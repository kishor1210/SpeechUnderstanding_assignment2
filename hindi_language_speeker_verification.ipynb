{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cs_jXHvrFc",
        "outputId": "97ebc21d-691c-43bf-af6e-70cf541a3f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/assignemet2_speech\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/assignemet2_speech"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import roc_curve\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Tokenizer, HubertModel\n",
        "# Check if torchaudio's sox_io backend is available\n",
        "if torchaudio.get_audio_backend() != 'sox_io':\n",
        "    torchaudio.set_audio_backend(\"sox_io\")"
      ],
      "metadata": {
        "id": "1XqcwpndSt8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e204d927-200b-4dd0-e3cd-6e5523fde801"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-28cb41438edc>:13: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  if torchaudio.get_audio_backend() != 'sox_io':\n",
            "<ipython-input-2-28cb41438edc>:14: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"sox_io\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVh0ezJl5CRD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through all files in the directory\n",
        "data_path = []\n",
        "directory = \"datasets/test_data/audio\"\n",
        "for filename in os.listdir(directory):\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    data_path.append(filename)"
      ],
      "metadata": {
        "id": "PQkxDq-28H3h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"filename\":data_path})\n"
      ],
      "metadata": {
        "id": "DtRZP1BIDYVy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gaDjsKFDDpwx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_id'] = df['filename'].apply(lambda x: '_'.join(x[:-4].split('-')[1:]))\n",
        "\n",
        "df1 = df.drop_duplicates(subset='user_id', keep='first').reset_index(drop=True)\n",
        "df2 = df.drop_duplicates(subset='user_id', keep='last').reset_index(drop=True)\n",
        "\n",
        "final_l1 = pd.DataFrame()\n",
        "final_l2 = pd.DataFrame()\n",
        "\n",
        "final_l1['audio_1'] = df1.sort_values(by='user_id',ascending=True)['filename'].values\n",
        "final_l1['audio_2'] = df2.sort_values(by='user_id',ascending=True)['filename'].values\n",
        "final_l1['level'] = 1\n",
        "\n",
        "final_l2['audio_1'] = df1.sort_values(by='user_id',ascending=True)['filename'].values\n",
        "final_l2['audio_2'] = df2.sort_values(by='user_id',ascending=False)['filename'].values\n",
        "final_l2['level'] = 0\n",
        "\n",
        "final_test = pd.concat([final_l1,final_l2],axis=0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "VHiSTqbiDENg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write DataFrame to a text file\n",
        "final_test.to_csv('data_test.txt', sep='\\t', index=False, header=False, line_terminator='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkTZrAftUw8e",
        "outputId": "bc475b59-bb13-4cad-ec04-6910ab63eb43"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-87c8ec2300c5>:2: FutureWarning: the 'line_terminator'' keyword is deprecated, use 'lineterminator' instead.\n",
            "  final_test.to_csv('data_test.txt', sep='\\t', index=False, header=False, line_terminator='\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ly922Ae8-ZIs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a dataset class to load the data\n",
        "class data_load(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            self.data = [line.split() for line in file.readlines()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "cvXGor91V_HB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Define your model architecture\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Load pre-trained model and tokenizer\n",
        "        model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
        "        #self.model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\").eval()\n",
        "        self.model = Wav2Vec2Model.from_pretrained(model_name).eval()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Assuming x1 and x2 are paths to audio files\n",
        "        # You need to implement how to load and process audio files into features\n",
        "        # Convert audio files to features\n",
        "        feature1 = self.preprocess_audio(x1)\n",
        "        feature2 = self.preprocess_audio(x2)\n",
        "\n",
        "        # Extract embeddings\n",
        "        with torch.no_grad():\n",
        "            out1 = self.model(feature1.unsqueeze(0)).last_hidden_state\n",
        "            out2 = self.model(feature2.unsqueeze(0)).last_hidden_state\n",
        "\n",
        "        # Flatten the embeddings\n",
        "        out1_emb = out1.squeeze(0)\n",
        "        out2_emb = out2.squeeze(0)\n",
        "\n",
        "        # Ensure the dimensions match\n",
        "        min_length = min(out1_emb.shape[0], out2_emb.shape[0])\n",
        "        output1 = out1_emb[:min_length]\n",
        "        output2 = out2_emb[:min_length]\n",
        "\n",
        "\n",
        "        # Here, you need to define how to compute similarity between output1 and output2\n",
        "        # For example, you can use cosine similarity, Euclidean distance, etc.\n",
        "        similarity_score = self.compute_similarity(output1, output2)\n",
        "\n",
        "        return similarity_score\n",
        "\n",
        "\n",
        "    # Function to preprocess audio clips\n",
        "    def preprocess_audio(self, audio_path):\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        TARGET_SAMPLE_RATE =16000\n",
        "        # Resample if necessary\n",
        "        if sample_rate != TARGET_SAMPLE_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # # Convert stereo to mono if necessary\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        #print(waveform.shape)\n",
        "        # Ensure single channel\n",
        "        waveform = waveform.squeeze(0)  # Remove batch dimension if present\n",
        "        #print(waveform.shape)\n",
        "        if waveform.dim() > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)  # Take the mean if multiple channels\n",
        "\n",
        "        # Normalize waveform\n",
        "        waveform /= torch.max(torch.abs(waveform))\n",
        "\n",
        "        return waveform\n",
        "\n",
        "\n",
        "    def compute_similarity(self, output1, output2):\n",
        "        # Implement similarity computation here\n",
        "        # For demonstration, let's assume we compute cosine similarity between output1 and output2\n",
        "        # You might need to reshape or process the outputs before computing similarity\n",
        "        # Here's a simple example of computing cosine similarity\n",
        "        # Note: This is just a placeholder. Implement the actual similarity computation as needed.\n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity(output1, output2, dim=1)\n",
        "        return similarity.mean().item()\n",
        "\n",
        "# Step 3: Calculate EER\n",
        "def calculate_eer(scores, labels):\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
        "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "    return eer   # Convert to percentage"
      ],
      "metadata": {
        "id": "M-YdjOMnyq85"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = data_load(\"data_test.txt\")\n",
        "\n",
        "# Initialize the model\n",
        "model = MyModel()\n",
        "\n",
        "# Load pre-trained weights if necessary\n",
        "# model.load_state_dict(torch.load('pretrained_model.pth'))\n",
        "\n",
        "# Lists to store scores and labels\n",
        "scores = []\n",
        "labels = []\n",
        "no_file = []"
      ],
      "metadata": {
        "id": "-j3IgpBPy2-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2dc5f3-a000-4118-b300-f5ab8b8408d9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXAjWagXbieh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataset and generate scores\n",
        "for data in dataset:\n",
        "    # Here, data[0] and data[1] represent the paths of two audio files for comparison\n",
        "    # You need to implement how you load and process these audio files\n",
        "    # Then, pass them through your model to get the similarity score\n",
        "    #try:\n",
        "    score = model('datasets/test_data/audio/'+data[0], 'datasets/test_data/audio/'+data[1])  # Adjust this line according to your model's input\n",
        "\n",
        "    scores.append(score)\n",
        "    labels.append(int(data[2]))"
      ],
      "metadata": {
        "id": "yBMfn9VAb5Qe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate EER for Wav2Vec2Model\n",
        "eer = calculate_eer(scores, labels)\n",
        "print(f\"EER: {eer:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIdpU9QgrZCv",
        "outputId": "93de8b29-5646-49af-b1cc-a0086f214869"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EER: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate EER for HubertModel\n",
        "eer = calculate_eer(scores, labels)\n",
        "print(f\"EER: {eer:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu5vGQ_mcNZ0",
        "outputId": "80a06e00-0b04-4633-d6de-dd394b42821f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EER: 0.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FT8El5khos3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}