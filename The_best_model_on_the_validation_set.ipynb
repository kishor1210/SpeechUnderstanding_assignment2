{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WepKPYvE5vtg",
        "outputId": "3969bd29-49ba-46e9-f44e-33ef1fc8643d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/assignemet2_speech\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/assignemet2_speech/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDr8vfaNsqlJ",
        "outputId": "674559a0-9493-4fa3-d9fd-73314bf097b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets  data_test.txt  hindi_language_speeker_verification.ipynb  Untitled0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import roc_curve\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Tokenizer, HubertModel\n",
        "# Check if torchaudio's sox_io backend is available\n",
        "if torchaudio.get_audio_backend() != 'sox_io':\n",
        "    torchaudio.set_audio_backend(\"sox_io\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNP1wMIfo6o_",
        "outputId": "6528ccfb-b004-4c5b-af60-6a2dc157e493"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-28cb41438edc>:13: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  if torchaudio.get_audio_backend() != 'sox_io':\n",
            "<ipython-input-16-28cb41438edc>:14: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"sox_io\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class KathbathDataset(Dataset):\n",
        "    def __init__(self, root_dir, split_name, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split_name = split_name\n",
        "        self.transform = transform\n",
        "        self.speakers = sorted(os.listdir(os.path.join(root_dir, split_name)))\n",
        "        self.audio_files = []\n",
        "        self.labels = []\n",
        "        self.audio_ids = []\n",
        "        self.user_ids = []\n",
        "        self.genders = []\n",
        "\n",
        "        for speaker in self.speakers:\n",
        "            speaker_dir = os.path.join(root_dir, split_name, speaker)\n",
        "            files = sorted(os.listdir(speaker_dir))\n",
        "            for file in files:\n",
        "                self.audio_files.append(os.path.join(speaker_dir, file))\n",
        "                self.labels.append(self.speakers.index(speaker))\n",
        "                audio_id, user_id, gender = self.parse_audio_filename(file)\n",
        "                self.audio_ids.append(audio_id)\n",
        "                self.user_ids.append(user_id)\n",
        "                self.genders.append(gender)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_files[idx]\n",
        "        label = self.labels[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "        return waveform, label\n",
        "\n",
        "    def parse_audio_filename(self, filename):\n",
        "        parts = filename.split('-')\n",
        "        audio_id = parts[0]\n",
        "        user_id = parts[1]\n",
        "        gender = parts[2].split('.')[0]\n",
        "        return audio_id, user_id, gender\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOPpP_7nr-xE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Define your model architecture\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Load pre-trained model and tokenizer\n",
        "        model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
        "        #self.model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\").eval()\n",
        "        self.model = Wav2Vec2Model.from_pretrained(model_name).eval()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Assuming x1 and x2 are paths to audio files\n",
        "        # You need to implement how to load and process audio files into features\n",
        "        # Convert audio files to features\n",
        "        feature1 = self.preprocess_audio(x1)\n",
        "        feature2 = self.preprocess_audio(x2)\n",
        "\n",
        "        # Extract embeddings\n",
        "        with torch.no_grad():\n",
        "            out1 = self.model(feature1.unsqueeze(0)).last_hidden_state\n",
        "            out2 = self.model(feature2.unsqueeze(0)).last_hidden_state\n",
        "\n",
        "        # Flatten the embeddings\n",
        "        out1_emb = out1.squeeze(0)\n",
        "        out2_emb = out2.squeeze(0)\n",
        "\n",
        "        # Ensure the dimensions match\n",
        "        min_length = min(out1_emb.shape[0], out2_emb.shape[0])\n",
        "        output1 = out1_emb[:min_length]\n",
        "        output2 = out2_emb[:min_length]\n",
        "\n",
        "\n",
        "        # Here, you need to define how to compute similarity between output1 and output2\n",
        "        # For example, you can use cosine similarity, Euclidean distance, etc.\n",
        "        similarity_score = self.compute_similarity(output1, output2)\n",
        "\n",
        "        return similarity_score\n",
        "\n",
        "\n",
        "    # Function to preprocess audio clips\n",
        "    def preprocess_audio(self, audio_path):\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        TARGET_SAMPLE_RATE =16000\n",
        "        # Resample if necessary\n",
        "        if sample_rate != TARGET_SAMPLE_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # # Convert stereo to mono if necessary\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "        #print(waveform.shape)\n",
        "        # Ensure single channel\n",
        "        waveform = waveform.squeeze(0)  # Remove batch dimension if present\n",
        "        #print(waveform.shape)\n",
        "        if waveform.dim() > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)  # Take the mean if multiple channels\n",
        "\n",
        "        # Normalize waveform\n",
        "        waveform /= torch.max(torch.abs(waveform))\n",
        "\n",
        "        return waveform\n",
        "\n",
        "\n",
        "    def compute_similarity(self, output1, output2):\n",
        "        # Implement similarity computation here\n",
        "        # For demonstration, let's assume we compute cosine similarity between output1 and output2\n",
        "        # You might need to reshape or process the outputs before computing similarity\n",
        "        # Here's a simple example of computing cosine similarity\n",
        "        # Note: This is just a placeholder. Implement the actual similarity computation as needed.\n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity(output1, output2, dim=1)\n",
        "        return similarity.mean().item()\n",
        "\n"
      ],
      "metadata": {
        "id": "O7LaS20PpWk-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming MyModel is defined as provided in the question\n",
        "# Assuming Kathbath Dataset has a structure compatible with PyTorch Dataset\n",
        "\n",
        "# Define a function to compute the EER\n",
        "def compute_eer(scores, labels):\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
        "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "    return eer * 100  # Convert to percentage\n",
        "\n",
        "# Load the Kathbath Dataset and define data loaders\n",
        "# Replace <kathbath_dataset_path> with the path to your Kathbath Dataset\n",
        "# Replace <batch_size> with the desired batch size\n",
        "kathbath_dataset_path = \"<kathbath_dataset_path>\"\n",
        "batch_size = <batch_size>\n",
        "\n",
        "# Assuming KathbathDataset class is implemented and DataLoader is used to load data\n",
        "# Example usage:\n",
        "train_dataset = KathbathDataset(root_dir=\"datasets\", split_name=\"valid_data\")\n",
        "val_dataset = KathbathDataset(root_dir=\"datasets\", split_name=\"valid_data\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "# Define the fine-tuning procedure\n",
        "def fine_tune_model(model, train_loader, val_loader, optimizer, criterion, num_epochs):\n",
        "    best_eer = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            # Forward pass\n",
        "            audio1, audio2, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            output = model(audio1, audio2)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        eer = evaluate_model(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation EER: {eer:.2f}%\")\n",
        "\n",
        "        # Save the model if it has the best EER so far\n",
        "        if eer < best_eer:\n",
        "            best_eer = eer\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    return best_eer\n",
        "\n",
        "# Define a function to evaluate the model on the validation set\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            audio1, audio2, batch_labels = batch\n",
        "            output = model(audio1, audio2)\n",
        "            scores.extend(output.cpu().numpy())\n",
        "            labels.extend(batch_labels.cpu().numpy())\n",
        "    eer = compute_eer(scores, labels)\n",
        "    return eer\n",
        "\n",
        "# Fine-tune the model\n",
        "# Define your optimizer and loss function\n",
        "# Replace <optimizer> and <criterion> with appropriate choices\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.BCELoss()  # Assuming binary classification (similar/dissimilar)\n",
        "num_epochs = 10  # Define the number of epochs for fine-tuning\n",
        "\n",
        "# Fine-tune the model\n",
        "best_eer = fine_tune_model(model, train_loader, val_loader, optimizer, criterion, num_epochs)\n",
        "\n",
        "print(f\"Best Validation EER: {best_eer:.2f}%\")\n",
        "\n",
        "# Once fine-tuning is done, you can evaluate the best model on the test set if available\n",
        "# Load the best model\n",
        "best_model = MyModel()\n",
        "best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "\n",
        "\n",
        "test_dataset = KathbathDataset(root_dir=\"datasets\", split_name=\"test_data\")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "# Evaluate the best model on the test set and report the EER\n",
        "test_eer = evaluate_model(best_model, test_loader)\n",
        "print(f\"Test EER: {test_eer:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RwVBR9Ny51sJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}